{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlFYXxOn6SxK",
        "outputId": "9724d9fe-325c-439a-92cd-b780d20fec16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. Force Mount Google Drive (Ensure reading the latest files)\n",
        "# ==========================================\n",
        "# force_remount=True is key! It forces a cache refresh,\n",
        "# ensuring the IMS.xlsx you just saved on your computer is read immediately by Colab.\n",
        "try:\n",
        "    drive.unmount('/content/drive')\n",
        "    print(\"âœ… Google Drive unmount attempted, preparing to remount...\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Google Drive unmount failed or not needed: {e}\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ==========================================\n",
        "# 2. Lock Working Directory (Prevent file loss)\n",
        "# ==========================================\n",
        "target_folder_name = 'err'  # Your target folder\n",
        "work_dir = f'/content/drive/MyDrive/{target_folder_name}'\n",
        "\n",
        "# Check and create folder\n",
        "if not os.path.exists(work_dir):\n",
        "    print(f\"âš ï¸ Folder '{target_folder_name}' does not exist, creating...\")\n",
        "    os.makedirs(work_dir)\n",
        "else:\n",
        "    print(f\"ðŸ“‚ Folder '{target_folder_name}' already exists.\")\n",
        "\n",
        "# Change directory\n",
        "try:\n",
        "    os.chdir(work_dir)\n",
        "    print(f\"âœ… Success! Directory changed to: {os.getcwd()}\")\n",
        "    print(\"   (Files forcibly refreshed, and new files will be saved here)\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to change directory: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4cOwiP794c_",
        "outputId": "b727698a-c068-4f8e-becf-b38d789bf341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Google Drive unmount failed or not needed: module 'google.colab.drive' has no attribute 'unmount'\n",
            "Mounted at /content/drive\n",
            "ðŸ“‚ Folder 'err' already exists.\n",
            "âœ… Success! Directory changed to: /content/drive/MyDrive/err\n",
            "   (Files forcibly refreshed, and new files will be saved here)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ› ï¸ File Sync Checker (Sync Checker)\n",
        "# ==========================================\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# 1. Re-confirm directory (Prevent failure to switch)\n",
        "target_folder_name = 'err'\n",
        "work_dir = f'/content/drive/MyDrive/{target_folder_name}'\n",
        "if os.getcwd() != work_dir:\n",
        "    try:\n",
        "        os.chdir(work_dir)\n",
        "        print(f\"ðŸ“‚ Directory corrected to: {os.getcwd()}\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Unable to switch directory, please check path.\")\n",
        "\n",
        "# 2. Find the latest IMS file\n",
        "files = [f for f in os.listdir('.') if f.endswith('.xlsx') and \"IMS\" in f and not f.startswith('~$')]\n",
        "files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "\n",
        "if files:\n",
        "    target_file = files[0]\n",
        "    # Get file timestamp\n",
        "    timestamp = os.path.getmtime(target_file)\n",
        "    file_time = datetime.datetime.fromtimestamp(timestamp)\n",
        "    current_time = datetime.datetime.now()\n",
        "\n",
        "    print(f\"ðŸ“„ Latest file seen by Colab: {target_file}\")\n",
        "    print(f\"ðŸ•’ File modification time: {file_time.strftime('%Y-%m-%d %H:%M:%S')} (Colab time)\")\n",
        "\n",
        "    # Simple check: If file time is more than 10 minutes ago, it might be old\n",
        "    time_diff = (current_time - file_time).total_seconds() / 60\n",
        "    print(f\"â±ï¸ Time since: {int(time_diff)} minutes ago\")\n",
        "\n",
        "    if time_diff > 10:\n",
        "        print(\"\\nâš ï¸ Warning: This file looks a bit old!\")\n",
        "        print(\"   ðŸ‘‰ Please confirm your Google Drive desktop app shows 'Sync Complete' (Green Check).\")\n",
        "        print(\"   ðŸ‘‰ Suggest waiting 10-30 seconds and clicking Run All again.\")\n",
        "else:\n",
        "    print(\"âŒ Error: No IMS Excel file found in current directory!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IsrzRLg-NOF",
        "outputId": "593f5cbe-a330-4860-d7b1-ba3aec964e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Latest file seen by Colab: IMS.xlsx\n",
            "ðŸ•’ File modification time: 2026-01-06 21:54:14 (Colab time)\n",
            "â±ï¸ Time since: 1327 minutes ago\n",
            "\n",
            "âš ï¸ Warning: This file looks a bit old!\n",
            "   ðŸ‘‰ Please confirm your Google Drive desktop app shows 'Sync Complete' (Green Check).\n",
            "   ðŸ‘‰ Suggest waiting 10-30 seconds and clicking Run All again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Kh2Iud1hbG",
        "outputId": "889b0771-3871-49a2-a456-9dca224d256c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "# ==========================================\n",
        "# Configuration Center (Configuration v29.1)\n",
        "# ==========================================\n",
        "VERSION = \"v29.1 (Feat: Overdue Auto-Correction)\"\n",
        "\n",
        "# Basic Parameters\n",
        "DEFAULT_INTERVAL = 7       # Default replenishment cycle (days)\n",
        "BLIND_SPOT_PENALTY = 1.5   # Penalty for stockouts\n",
        "Z_SCORE = 1.28             # Safety stock coefficient (Service Level)\n",
        "MIN_BUFFER_QTY = 0\n",
        "\n",
        "# Threshold Parameters\n",
        "HARD_CAP = 8               # Physical Hard Cap\n",
        "LEGACY_CUTOFF_DATE = \"2025-12-01\" # [v28.4] Cutoff date for legacy strategy\n",
        "\n",
        "# [v27.4] Smart Frequency Reduction Parameters\n",
        "HOT_SES_THRESHOLD = 2.0    # Hot item definition (SES >= 2.0)\n",
        "SAFE_COVERAGE_PCT = 0.80   # Safety coverage requirement (80%)\n",
        "MAX_HOT_RISK_ALLOWED = 1   # Max tolerated hot item stockouts\n",
        "\n",
        "# Rookie Phase Strategy\n",
        "ROOKIE_THRESHOLD_TOTAL = 5 # Total rookie phase duration (visits)\n",
        "PHASE_1_LIMIT = 2          # Phase 1 (Hard rule limit)\n",
        "\n",
        "# Forecasting Parameters\n",
        "SES_SPAN = 6               # SES Smoothing Span\n",
        "\n",
        "# File Paths\n",
        "SHADOW_LEDGER_FILE = \"shadow_ledger.json\"\n",
        "SKU_HISTORY_FILE = \"sku_history_tracker.json\"\n",
        "GRAVEYARD_FILE = \"sku_graveyard.json\"\n",
        "VISIT_HISTORY_FILE = \"visit_history.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_utils.py\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# Utilities\n",
        "# ==========================================\n",
        "\n",
        "def parse_strict_date(val):\n",
        "    if isinstance(val, (datetime.datetime, pd.Timestamp)):\n",
        "        return val.to_pydatetime() if isinstance(val, pd.Timestamp) else val\n",
        "    val_str = str(val).strip().replace('.0', '')\n",
        "    match = re.search(r'(\\d+)', val_str)\n",
        "    if not match: return None\n",
        "    clean_str = match.group(1)\n",
        "    if len(clean_str) == 7: clean_str = '0' + clean_str\n",
        "    if len(clean_str) != 8: return None\n",
        "    try: return datetime.datetime.strptime(clean_str, \"%d%m%Y\")\n",
        "    except: return None\n",
        "\n",
        "def load_json(filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_json(filepath, data):\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def load_sku_map(file_path):\n",
        "    \"\"\"\n",
        "    [v27.2] Read SKU real categories from Excel\n",
        "    \"\"\"\n",
        "    sku_map = {}\n",
        "    try:\n",
        "        xls = pd.ExcelFile(file_path)\n",
        "        # Find sheet containing SKU\n",
        "        sku_sheet = next((s for s in xls.sheet_names if 'sku' in s.lower()), None)\n",
        "\n",
        "        if sku_sheet:\n",
        "            # Read data (assuming no header or fixed structure)\n",
        "            df = pd.read_excel(xls, sheet_name=sku_sheet, header=None)\n",
        "\n",
        "            # Iterate from row 2 (skip header)\n",
        "            # Col 0: Category (Snack, Chocolate, Bakery...)\n",
        "            # Col 5: Name (Lay'S_Classic...)\n",
        "            for idx, row in df.iloc[2:].iterrows():\n",
        "                if pd.notna(row[5]) and pd.notna(row[0]):\n",
        "                    name = str(row[5]).strip().lower()\n",
        "                    raw_cat = str(row[0]).strip().lower()\n",
        "\n",
        "                    # Mapping Logic\n",
        "                    if 'bakery' in raw_cat:\n",
        "                        final_cat = 'Bakery'\n",
        "                    elif 'chocolate' in raw_cat or 'gummy' in raw_cat:\n",
        "                        final_cat = 'Chocolate'\n",
        "                    elif 'snack' in raw_cat:\n",
        "                        final_cat = 'Chips'\n",
        "                    else:\n",
        "                        final_cat = 'Chips' # Default fallback\n",
        "\n",
        "                    sku_map[name] = final_cat\n",
        "            print(f\"âœ… SKU Map loaded successfully: Contains {len(sku_map)} products\")\n",
        "            return sku_map\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ SKU Map load failed: {e}\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "def get_category_type(product_name, sku_map=None):\n",
        "    \"\"\"\n",
        "    [v27.2] Prefer table lookup, fallback to guessing\n",
        "    \"\"\"\n",
        "    name_lower = str(product_name).lower()\n",
        "\n",
        "    # 1. Lookup (Precise)\n",
        "    if sku_map and name_lower in sku_map:\n",
        "        return sku_map[name_lower]\n",
        "\n",
        "    # 2. Keyword Guessing (Fallback)\n",
        "    if 'bakery' in name_lower or 'cookie' in name_lower: return 'Bakery'\n",
        "    if 'chocolate' in name_lower or 'kit kat' in name_lower or 'mars' in name_lower: return 'Chocolate'\n",
        "    return 'Chips'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN5HQ5Sl1i0H",
        "outputId": "c945a505-3582-4ece-a5d4-c1d821038c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile storage_manager.py\n",
        "import datetime\n",
        "from data_utils import load_json, save_json\n",
        "import config\n",
        "\n",
        "# ==========================================\n",
        "# Archivist (File Management)\n",
        "# ==========================================\n",
        "\n",
        "def update_shadow_ledger_entry(shadow_data, loc, sku, date_str, qty, cap):\n",
        "    \"\"\"Update Shadow Ledger: Record virtual stock and Cap settings\"\"\"\n",
        "    if loc not in shadow_data: shadow_data[loc] = {}\n",
        "    if sku not in shadow_data[loc]: shadow_data[loc][sku] = []\n",
        "\n",
        "    found = False\n",
        "    for entry in shadow_data[loc][sku]:\n",
        "        if entry['date'] == date_str:\n",
        "            entry['qty'] = qty\n",
        "            entry['cap'] = cap\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        shadow_data[loc][sku].append({'date': date_str, 'qty': qty, 'cap': cap})\n",
        "\n",
        "def update_archives_and_save(shadow_ledger, advisor_data):\n",
        "    \"\"\"\n",
        "    Update all backend archives:\n",
        "    1. Shadow Ledger\n",
        "    2. SKU History\n",
        "    3. Graveyard\n",
        "    4. Visit History\n",
        "    \"\"\"\n",
        "    # 1. Save Shadow Ledger\n",
        "    save_json(config.SHADOW_LEDGER_FILE, shadow_ledger)\n",
        "\n",
        "    # Load other archives\n",
        "    sku_history = load_json(config.SKU_HISTORY_FILE)\n",
        "    graveyard = load_json(config.GRAVEYARD_FILE)\n",
        "    visit_history = load_json(config.VISIT_HISTORY_FILE)\n",
        "\n",
        "    today_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    for loc_name, loc_data in advisor_data.items():\n",
        "        current_stats = loc_data['sku_stats']\n",
        "        current_drop_qty = loc_data['total_drop_qty']\n",
        "\n",
        "        # 2. Update Visit History (For frequency reduction logic next time)\n",
        "        visit_history[loc_name] = {'last_drop_qty': current_drop_qty, 'date': today_str}\n",
        "\n",
        "        # 3. Zombie Check (Graveyard Check)\n",
        "        # If in old list but not in new list, it was removed\n",
        "        if loc_name in sku_history:\n",
        "            for old_sku, old_ses in sku_history[loc_name].items():\n",
        "                if old_sku not in current_stats:\n",
        "                    if loc_name not in graveyard: graveyard[loc_name] = []\n",
        "                    # Avoid duplicate records for the same day\n",
        "                    if not any(g['name'] == old_sku and g['date'] == today_str for g in graveyard[loc_name]):\n",
        "                        graveyard[loc_name].append({\n",
        "                            'name': old_sku,\n",
        "                            'lost_ses': old_ses,\n",
        "                            'date': today_str\n",
        "                        })\n",
        "                        print(f\"âš°ï¸  [Archive] {loc_name}: Detected removal of {old_sku} (Hist SES {old_ses:.2f})\")\n",
        "\n",
        "        # 4. Update SKU History\n",
        "        # Overwrite old data with new SES\n",
        "        sku_history[loc_name] = {k: v['ses'] for k, v in current_stats.items()}\n",
        "\n",
        "    # Save all changes\n",
        "    save_json(config.SKU_HISTORY_FILE, sku_history)\n",
        "    save_json(config.GRAVEYARD_FILE, graveyard)\n",
        "    save_json(config.VISIT_HISTORY_FILE, visit_history)\n",
        "\n",
        "    print(\"âœ… All background archives safely updated (Shadow, History, Graveyard, Visit).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOZ9jZyqAD2z",
        "outputId": "6af9e2c2-203d-4d10-9d28-d467fd018335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting storage_manager.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile advisor.py\n",
        "import config\n",
        "\n",
        "# ==========================================\n",
        "# Operations Advisor (Advisor v29.0)\n",
        "# ==========================================\n",
        "class OperationsAdvisor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def analyze(self, sku_stats, interval_days, is_rookie=False):\n",
        "        \"\"\"\n",
        "        [v29.0] Only give advice for mature locations. Silence during rookie phase.\n",
        "        \"\"\"\n",
        "        # 1. If rookie phase, return empty, keep quiet\n",
        "        if is_rookie:\n",
        "            return []\n",
        "\n",
        "        advice_list = []\n",
        "        total_skus = len(sku_stats)\n",
        "        if total_skus == 0: return []\n",
        "\n",
        "        safe_count = 0\n",
        "        hot_risk_list = []\n",
        "\n",
        "        for name, info in sku_stats.items():\n",
        "            ses = info['ses']\n",
        "            final_stock = info.get('final_stock', 0)\n",
        "\n",
        "            daily_sales = ses / 7.0\n",
        "            demand = daily_sales * interval_days\n",
        "\n",
        "            is_safe = (final_stock >= demand) or (final_stock >= config.HARD_CAP)\n",
        "            if is_safe: safe_count += 1\n",
        "\n",
        "            if ses >= config.HOT_SES_THRESHOLD and not is_safe:\n",
        "                hot_risk_list.append(f\"{name}({ses:.1f})\")\n",
        "\n",
        "        coverage_pct = safe_count / total_skus\n",
        "        hot_risk_count = len(hot_risk_list)\n",
        "\n",
        "        # Only keep critical health info, minimize chatter\n",
        "        risk_msg = \"None\" if hot_risk_count == 0 else f\"{hot_risk_count} SKUs ({', '.join(hot_risk_list[:2])}...)\"\n",
        "        advice_list.append(f\"ðŸ“Š [Health] Coverage: {coverage_pct:.0%} | Risk: {risk_msg}\")\n",
        "\n",
        "        hot_candidates = {}\n",
        "        cold_candidates = {}\n",
        "\n",
        "        for name, info in sku_stats.items():\n",
        "            cat = info['category']\n",
        "            streak = info['stockout_streak']\n",
        "            cap = info.get('current_cap', 4)\n",
        "            ses = info['ses']\n",
        "\n",
        "            # Report severe hot items\n",
        "            if streak >= 2 and cap >= 8:\n",
        "                if cat not in hot_candidates: hot_candidates[cat] = []\n",
        "                hot_candidates[cat].append(name)\n",
        "                advice_list.append(f\"ðŸ”¥ [Hot] {name}: Stockout {streak}x -> Suggest Adding Facings\")\n",
        "\n",
        "            # Report severe cold items (non-rookie)\n",
        "            elif 0 < ses < 0.3 and streak == 0:\n",
        "                advice_list.append(f\"â„ï¸ [Cold] {name}: Daily Sales only {ses:.2f} -> Suggest Delisting\")\n",
        "\n",
        "        return advice_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rigJzyaAG2p",
        "outputId": "b1e66541-e014-41b0-d6fa-b373cc41ee34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting advisor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile efficiency_checker.py\n",
        "import config\n",
        "\n",
        "# ==========================================\n",
        "# Backtest Referee (Efficiency Checker v29.0)\n",
        "# Responsibility: Review accuracy of the last replenishment\n",
        "# ==========================================\n",
        "\n",
        "class EfficiencyChecker:\n",
        "    def __init__(self):\n",
        "        self.total_sales = 0.0\n",
        "        self.total_waste = 0.0  # Over-Carry\n",
        "        self.total_missed = 0.0 # Under-Carry\n",
        "        self.sku_count = 0\n",
        "\n",
        "    def analyze_sku(self, last_real_record, ses_weekly, interval_days):\n",
        "        \"\"\"\n",
        "        Analyze single SKU performance from last time\n",
        "        last_real_record: contains {'stock'(Carry), 'rest', 'cons'}\n",
        "        ses_weekly: Calculated smooth weekly sales (reference for potential)\n",
        "        \"\"\"\n",
        "        if not last_real_record: return\n",
        "\n",
        "        carried = last_real_record['stock'] # Amount carried last time (Cap)\n",
        "        rest = last_real_record['rest']     # Amount remaining\n",
        "        sales = last_real_record['cons']    # Actual sales\n",
        "\n",
        "        # 1. Calculate Waste (Over-Carry)\n",
        "        # Anything remaining is waste\n",
        "        waste = rest\n",
        "\n",
        "        # 2. Calculate Missed (Under-Carry)\n",
        "        # Only calculate missed if stockout occurred (Rest=0)\n",
        "        missed = 0.0\n",
        "        if rest == 0:\n",
        "            # Estimate real demand\n",
        "            daily_demand = ses_weekly / 7.0\n",
        "            total_demand = daily_demand * interval_days\n",
        "\n",
        "            # Missed = Theoretical Demand - Actual Carried\n",
        "            # If Theoretical < Carried (Random bulk buy), missed is 0\n",
        "            missed = max(0, total_demand - carried)\n",
        "\n",
        "        self.total_sales += sales\n",
        "        self.total_waste += waste\n",
        "        self.total_missed += missed\n",
        "        self.sku_count += 1\n",
        "\n",
        "    def get_score(self):\n",
        "        \"\"\"\n",
        "        Output final score and stats\n",
        "        Formula: Efficiency = Sales / (Sales + Waste + Missed)\n",
        "        \"\"\"\n",
        "        if self.sku_count == 0:\n",
        "            return None\n",
        "\n",
        "        denominator = self.total_sales + self.total_waste + self.total_missed\n",
        "\n",
        "        if denominator == 0:\n",
        "            return {\n",
        "                \"score_pct\": 0.0,\n",
        "                \"waste\": self.total_waste,\n",
        "                \"missed\": self.total_missed,\n",
        "                \"rating\": \"N/A\" # All zeros\n",
        "            }\n",
        "\n",
        "        score = self.total_sales / denominator\n",
        "\n",
        "        # Simple Rating\n",
        "        if score >= 0.85: rating = \"â­ Excellent\"\n",
        "        elif score >= 0.70: rating = \"âœ… Good\"\n",
        "        elif score >= 0.50: rating = \"âš ï¸ Fair\"\n",
        "        else: rating = \"âŒ Poor\"\n",
        "\n",
        "        return {\n",
        "            \"score_pct\": score,\n",
        "            \"waste\": self.total_waste,\n",
        "            \"missed\": self.total_missed,\n",
        "            \"rating\": rating\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYAUdtPKAIuK",
        "outputId": "09ba8af4-cb41-4b6b-a041-2b4f16ab7b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting efficiency_checker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forecast_engine.py\n",
        "import pandas as pd\n",
        "import config\n",
        "\n",
        "# ==========================================\n",
        "# Forecast Engine\n",
        "# Responsibility: Pure math, calculates SES and Std Dev\n",
        "# ==========================================\n",
        "\n",
        "def calculate_forecast(history, is_sparse):\n",
        "    \"\"\"\n",
        "    Corresponds to original Layer 1 logic\n",
        "    Calculates Exponential Weighted Moving Average (EWMA) and Standard Deviation\n",
        "    \"\"\"\n",
        "    if not history: return 0.0, 0.0\n",
        "\n",
        "    series = pd.Series(history)\n",
        "    ewm_mean = series.ewm(span=config.SES_SPAN, adjust=False).mean().iloc[-1]\n",
        "    std = series.std() if len(series) > 1 else 0.0\n",
        "\n",
        "    # Sparse data / Stockout blind spot correction\n",
        "    if is_sparse:\n",
        "        max_reasonable_ses = (config.HARD_CAP * 1.5) / 7.0\n",
        "        if ewm_mean < max_reasonable_ses:\n",
        "            std *= config.BLIND_SPOT_PENALTY\n",
        "            ewm_mean = min(ewm_mean * 1.2, max_reasonable_ses)\n",
        "\n",
        "    return max(0, ewm_mean), std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeBHi2GIAKTy",
        "outputId": "6da33664-0708-4c10-9f63-0b7d67e2efab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting forecast_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile policy_engine.py\n",
        "import numpy as np\n",
        "import config\n",
        "from data_utils import get_category_type\n",
        "\n",
        "# ==========================================\n",
        "# Policy Engine (Policy Engine v28.7)\n",
        "# Responsibility: Business rules, Rookie phase, Cap setting, Qty decision\n",
        "# ==========================================\n",
        "\n",
        "def apply_policy(p_name, mean, std, prev_cap, count, last_rest, sku_map, chip_idx_in_loc=0, is_legacy_mode=False):\n",
        "    \"\"\"\n",
        "    [v28.7] Rookie Phase Logic Refactor: Simple promotion/relegation based on combat performance\n",
        "    \"\"\"\n",
        "    cat = get_category_type(p_name, sku_map)\n",
        "    policy = {}\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. Highest Priority: Legacy Lock\n",
        "    # ==========================================\n",
        "    if is_legacy_mode and cat == 'Chips':\n",
        "        policy = {\n",
        "            \"current_cap\": prev_cap,\n",
        "            \"trigger\": 999,\n",
        "            \"target\": prev_cap,\n",
        "            \"is_rookie\": False,\n",
        "            \"previous_cap\": prev_cap,\n",
        "            \"note\": \"Legacy Lock\"\n",
        "        }\n",
        "        return _apply_safety_net(policy, cat)\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. Rookie Phase Strategy\n",
        "    # ==========================================\n",
        "    if count < config.ROOKIE_THRESHOLD_TOTAL:\n",
        "\n",
        "        # --- Phase 1: Initial Layout (Visits 1-2) ---\n",
        "        # Logic: Fixed by position (First 7 Chips get 8, rest 4)\n",
        "        if count < config.PHASE_1_LIMIT:\n",
        "            target = 4; current_cap = 4\n",
        "            if cat == 'Chocolate': target = 8; current_cap = 8\n",
        "            elif cat == 'Bakery': target = 4; current_cap = 4\n",
        "            elif cat == 'Chips':\n",
        "                if chip_idx_in_loc <= 6: target = 8; current_cap = 8\n",
        "                else: target = 4; current_cap = 4\n",
        "            policy = {\"current_cap\": current_cap, \"trigger\": 999, \"target\": target, \"is_rookie\": True, \"previous_cap\": prev_cap}\n",
        "\n",
        "        # --- Phase 2: Combat Adjustment (Visits 3-5) [v28.7 New Logic] ---\n",
        "        # Logic: Look at last performance, simple promotion/relegation\n",
        "        else:\n",
        "            # 1. Calculate last sales (approximate)\n",
        "            # If last Cap was 8, rest 5, then sold 3\n",
        "            last_sales = max(0, prev_cap - last_rest)\n",
        "\n",
        "            # Default to inheriting last Cap\n",
        "            new_cap = prev_cap\n",
        "\n",
        "            # Only adjust Chips (Bakery/Choc locked by safety net later)\n",
        "            if cat == 'Chips':\n",
        "                # Rule A: Promotion (Cap 4 -> 8)\n",
        "                # Condition: Last was 4 AND Stockout (Rest=0)\n",
        "                if prev_cap == 4 and last_rest == 0:\n",
        "                    new_cap = 8\n",
        "\n",
        "                # Rule B: Relegation (Cap 8 -> 4)\n",
        "                # Condition: Last was 8 AND Remaining(Rest) > Replenished/Sold(Sales)\n",
        "                # Note: \"Replenished\" assumes \"Sold\" (assuming full refill last time)\n",
        "                elif prev_cap == 8 and last_rest > last_sales:\n",
        "                    new_cap = 4\n",
        "\n",
        "                # Rule C: Maintain\n",
        "                # (If conditions not met, new_cap remains prev_cap)\n",
        "\n",
        "            # Set Target\n",
        "            # Rookie phase fills to Cap\n",
        "            target = new_cap\n",
        "            current_cap = new_cap\n",
        "\n",
        "            policy = {\"current_cap\": current_cap, \"trigger\": 999, \"target\": target, \"is_rookie\": True, \"previous_cap\": prev_cap}\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. Mature Phase Strategy\n",
        "    # ==========================================\n",
        "    else:\n",
        "        # Dynamic Cap Adjustment\n",
        "        if cat == 'Chips':\n",
        "             curr_cap = 8 if mean >= 2.0 else 4\n",
        "        else:\n",
        "             curr_cap = 4 # Default\n",
        "\n",
        "        # Chips Relegation Protection (Retained in Mature Phase)\n",
        "        if cat == 'Chips' and prev_cap > 0: pass\n",
        "        else:\n",
        "            if cat == 'Chips': prev_cap = 4\n",
        "\n",
        "        buffer = (config.Z_SCORE * std) + config.MIN_BUFFER_QTY\n",
        "        trigger = mean + buffer\n",
        "        target = min(trigger, curr_cap)\n",
        "        if target < 1: target = 1\n",
        "\n",
        "        policy = {\n",
        "            \"current_cap\": curr_cap,\n",
        "            \"trigger\": trigger,\n",
        "            \"target\": target,\n",
        "            \"is_rookie\": False,\n",
        "            \"raw_forecast\": mean,\n",
        "            \"previous_cap\": prev_cap\n",
        "        }\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. Final Safety Net\n",
        "    # ==========================================\n",
        "    # This prevents Cookies from accidentally being set to 8, or Chocolate to 4\n",
        "    return _apply_safety_net(policy, cat)\n",
        "\n",
        "def _apply_safety_net(policy, cat):\n",
        "    \"\"\"\n",
        "    Regardless of previous logic, apply final mandatory correction here.\n",
        "    \"\"\"\n",
        "    # Bakery must be <= 4\n",
        "    if cat == 'Bakery':\n",
        "        policy['current_cap'] = 4\n",
        "        if policy['target'] > 4: policy['target'] = 4\n",
        "\n",
        "    # Chocolate must be <= 8\n",
        "    elif cat == 'Chocolate':\n",
        "        policy['current_cap'] = 8\n",
        "        if policy['target'] > 8: policy['target'] = 8\n",
        "\n",
        "    # Global Hard Cap\n",
        "    if policy['target'] > config.HARD_CAP: policy['target'] = config.HARD_CAP\n",
        "\n",
        "    return policy\n",
        "\n",
        "def make_decision(start, is_fill, days, policy):\n",
        "    # Rookie Phase or Legacy Lock -> Fill to Target directly\n",
        "    if policy.get('is_rookie', False) or policy.get('note') == 'Legacy Lock':\n",
        "        return int(np.ceil(policy['target'])), None, policy['current_cap']\n",
        "\n",
        "    burn = policy['raw_forecast'] / 7.0\n",
        "    est_stock = start - (burn * days)\n",
        "\n",
        "    if est_stock > policy['trigger']:\n",
        "        return 0, max(0, est_stock - policy['raw_forecast']), policy['previous_cap']\n",
        "\n",
        "    return int(np.ceil(policy['target'])), None, policy['current_cap']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGuoHBw2AUEK",
        "outputId": "9cb3f96b-ace9-4a51-99e0-a3f8e1b46ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting policy_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_logic.py\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import config\n",
        "from data_utils import get_category_type, parse_strict_date\n",
        "from storage_manager import update_shadow_ledger_entry\n",
        "from advisor import OperationsAdvisor\n",
        "\n",
        "# Import Engines\n",
        "from forecast_engine import calculate_forecast\n",
        "from policy_engine import apply_policy, make_decision\n",
        "# [v29.0] Import Efficiency Checker\n",
        "from efficiency_checker import EfficiencyChecker\n",
        "\n",
        "# ==========================================\n",
        "# Core Processor (Core Processor v29.1)\n",
        "# ==========================================\n",
        "\n",
        "def process_single_location_block(df_block, location_name, date_row, sku_map, loc_intervals, manual_plan, shadow_ledger, advisor_data_collector, current_interval=7, dry_run=False, target_date_override=None):\n",
        "    # --- Date Parsing ---\n",
        "    col_dates = {}\n",
        "    for i in range(len(date_row)):\n",
        "        raw_val = date_row.iloc[i]\n",
        "        d = parse_strict_date(raw_val)\n",
        "        if d:\n",
        "            if i > 0: col_dates[i-1] = d\n",
        "            col_dates[i] = d\n",
        "\n",
        "    valid_dates = list(col_dates.values())\n",
        "    valid_dates_set = set(d.strftime(\"%Y-%m-%d\") for d in valid_dates)\n",
        "    last_visit_date = max(valid_dates) if valid_dates else None\n",
        "    last_visit_str = last_visit_date.strftime(\"%Y-%m-%d\") if last_visit_date else \"N/A\"\n",
        "\n",
        "    # [v29.1] Target Date Logic Update\n",
        "    # Prefer override date from main.py (handling Overdue)\n",
        "    # Fallback to default logic if not provided\n",
        "    if target_date_override:\n",
        "        target_date = target_date_override\n",
        "    else:\n",
        "        target_date = (last_visit_date + timedelta(days=7)) if last_visit_date else datetime.datetime.now()\n",
        "\n",
        "    target_date_iso = target_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # --- Legacy Strategy Detection ---\n",
        "    legacy_map = {}\n",
        "    col_indices = list(range(2, len(df_block.columns)-1, 2))\n",
        "    valid_col_indices = [i for i in col_indices if col_dates.get(i)]\n",
        "\n",
        "    try: legacy_cutoff = datetime.datetime.strptime(config.LEGACY_CUTOFF_DATE, \"%Y-%m-%d\")\n",
        "    except: legacy_cutoff = datetime.datetime(2025, 12, 1)\n",
        "\n",
        "    for i in valid_col_indices:\n",
        "        curr_date = col_dates.get(i)\n",
        "        is_legacy_day = True\n",
        "        has_chips_data = False\n",
        "        if curr_date and curr_date >= legacy_cutoff: is_legacy_day = False\n",
        "        else:\n",
        "            for idx in range(1, len(df_block)):\n",
        "                row = df_block.iloc[idx]\n",
        "                p_name = str(row.iloc[1])\n",
        "                if pd.isna(p_name) or p_name == 'nan': continue\n",
        "                cat = get_category_type(p_name, sku_map)\n",
        "                c_val = row.iloc[i]\n",
        "                if pd.notna(c_val):\n",
        "                    try:\n",
        "                        qty = float(c_val)\n",
        "                        if cat == 'Chips':\n",
        "                            has_chips_data = True\n",
        "                            if qty != 2.0 and qty != 6.0:\n",
        "                                is_legacy_day = False; break\n",
        "                    except: continue\n",
        "        if has_chips_data and is_legacy_day: legacy_map[i] = True\n",
        "        else: legacy_map[i] = False\n",
        "\n",
        "    # --- Main Loop Initialization ---\n",
        "    results = []\n",
        "    full_scan_list = []\n",
        "    if location_name not in shadow_ledger: shadow_ledger[location_name] = {}\n",
        "    current_loc_stats = {}\n",
        "    total_drop_qty = 0\n",
        "    period_stats = {}\n",
        "    chip_counter = 0\n",
        "    valid_dates_found = set()\n",
        "    history_count = len(valid_col_indices)\n",
        "\n",
        "    # [v29.0] Initialize Efficiency Checker\n",
        "    efficiency_checker = EfficiencyChecker()\n",
        "    is_rookie_loc = history_count < config.ROOKIE_THRESHOLD_TOTAL\n",
        "\n",
        "    # --- Iterate Products ---\n",
        "    for idx in range(1, len(df_block)):\n",
        "        row = df_block.iloc[idx]\n",
        "        p_name_raw = row.iloc[1]\n",
        "        if pd.isna(p_name_raw): continue\n",
        "        p_name = str(p_name_raw).strip()\n",
        "        if p_name.lower() == 'nan' or p_name == \"\" or \"Product\" in p_name: continue\n",
        "\n",
        "        timeline = []\n",
        "        stockout_streak = 0\n",
        "        cat_type = get_category_type(p_name, sku_map)\n",
        "        if cat_type == 'Chips': chip_counter += 1\n",
        "\n",
        "        sku_shadow_map = {}\n",
        "        if p_name in shadow_ledger[location_name]:\n",
        "            for entry in shadow_ledger[location_name][p_name]:\n",
        "                sku_shadow_map[entry['date']] = entry\n",
        "\n",
        "        sorted_indices = sorted(valid_col_indices, key=lambda x: col_dates[x])\n",
        "        last_valid_col_idx = -1\n",
        "\n",
        "        for k, i in enumerate(sorted_indices):\n",
        "            c_val = row.iloc[i]; r_val = row.iloc[i+1]\n",
        "            curr_date = col_dates.get(i)\n",
        "            if pd.notna(c_val) and pd.notna(r_val):\n",
        "                valid_dates_found.add(curr_date)\n",
        "                last_valid_col_idx = i\n",
        "                try:\n",
        "                    c = float(c_val); r = float(r_val)\n",
        "                    prev_date_iso = curr_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "                    if legacy_map.get(i, False):\n",
        "                        if cat_type == 'Chips': prev_cap = c\n",
        "                        elif c == 4.0: prev_cap = 4.0\n",
        "                        else: prev_cap = c\n",
        "                    else:\n",
        "                        if prev_date_iso in sku_shadow_map: prev_cap = sku_shadow_map[prev_date_iso].get('cap', c)\n",
        "                        else: prev_cap = c\n",
        "\n",
        "                    if r > prev_cap: cons = 0\n",
        "                    else:\n",
        "                        cons = max(0, prev_cap - r)\n",
        "                        if r == 0: cons *= config.BLIND_SPOT_PENALTY\n",
        "\n",
        "                    timeline.append({'date': curr_date, 'type': 'REAL', 'cons': cons, 'stock': prev_cap, 'rest': r})\n",
        "\n",
        "                    if i not in period_stats:\n",
        "                        days_span = 0\n",
        "                        if k > 0:\n",
        "                            prev_col_idx = sorted_indices[k-1]\n",
        "                            prev_d = col_dates.get(prev_col_idx)\n",
        "                            if prev_d: days_span = (curr_date - prev_d).days\n",
        "                        period_stats[i] = {'sales': 0.0, 'days': days_span, 'date': curr_date}\n",
        "                    period_stats[i]['sales'] += cons\n",
        "\n",
        "                except ValueError: continue\n",
        "\n",
        "        if p_name in shadow_ledger[location_name]:\n",
        "            for entry in shadow_ledger[location_name][p_name]:\n",
        "                try:\n",
        "                    d = datetime.datetime.strptime(entry['date'], \"%Y-%m-%d\")\n",
        "                    if entry['date'] in valid_dates_set: continue\n",
        "                    if d < target_date: timeline.append({'date': d, 'type': 'VIRTUAL', 'cons': 0, 'stock': entry['qty'], 'rest': entry['qty'], 'cap': entry.get('cap', 4)})\n",
        "                except: pass\n",
        "\n",
        "        timeline.sort(key=lambda x: x['date'])\n",
        "        for item in reversed(timeline):\n",
        "            if item['type'] == 'REAL':\n",
        "                if item['rest'] == 0: stockout_streak += 1\n",
        "                else: break\n",
        "\n",
        "        vel_history = []\n",
        "        is_sparse = False\n",
        "        if len(timeline) > 1:\n",
        "            for i in range(1, len(timeline)):\n",
        "                curr = timeline[i]; prev = timeline[i-1]\n",
        "                days = (curr['date'] - prev['date']).days\n",
        "                if days <= 0: continue\n",
        "                daily_rate = (curr['cons']/days)\n",
        "                if daily_rate > 3.0: daily_rate = 3.0\n",
        "                vel_history.append(daily_rate * 7.0)\n",
        "                if curr['type'] == 'VIRTUAL': is_sparse = True\n",
        "\n",
        "        mean, std = calculate_forecast(vel_history, is_sparse)\n",
        "\n",
        "        last = timeline[-1] if timeline else None\n",
        "        prev_cap_for_policy = 4\n",
        "        if last: prev_cap_for_policy = last.get('cap', last.get('stock', 4))\n",
        "        start = last['stock'] if last else 0\n",
        "        last_rest = last['rest'] if last else 0\n",
        "\n",
        "        if last and last['type'] == 'REAL' and last['stock'] > 4:\n",
        "            if not legacy_map.get(last_valid_col_idx, False): prev_cap_for_policy = 8\n",
        "\n",
        "        is_legacy_mode = False\n",
        "        if cat_type == 'Chips' and last_valid_col_idx != -1:\n",
        "            if legacy_map.get(last_valid_col_idx, False): is_legacy_mode = True\n",
        "\n",
        "        policy = apply_policy(p_name, mean, std, prev_cap_for_policy, len(vel_history), last_rest, sku_map, chip_counter, is_legacy_mode)\n",
        "\n",
        "        # [v29.1] Critical Fix: 'days' passed here is for burn calculation\n",
        "        # Use (target_date - last_visit_date) for real span\n",
        "        # If overdue, target_date is today, days will be large, burn large, qty large\n",
        "        days_diff = 7 # default\n",
        "        if last and last['date']:\n",
        "            days_diff = (target_date - last['date']).days\n",
        "            if days_diff < 1: days_diff = 7\n",
        "\n",
        "        qty, v_stock, res_cap = make_decision(start, True, days_diff, policy)\n",
        "\n",
        "        if qty > config.HARD_CAP: qty = config.HARD_CAP\n",
        "\n",
        "        final_stock_for_advisor = policy['target'] if qty > 0 else start\n",
        "        current_loc_stats[str(p_name).strip()] = {\n",
        "            'ses': mean,\n",
        "            'category': cat_type,\n",
        "            'stockout_streak': stockout_streak,\n",
        "            'current_cap': policy['current_cap'],\n",
        "            'final_stock': final_stock_for_advisor\n",
        "        }\n",
        "\n",
        "        full_scan_list.append([p_name, qty, policy['current_cap']])\n",
        "\n",
        "        if not dry_run:\n",
        "            update_shadow_ledger_entry(shadow_ledger, location_name, p_name, target_date_iso, v_stock if v_stock is not None else (start + qty), res_cap)\n",
        "            if qty > 0:\n",
        "                results.append([p_name, qty, policy['current_cap']])\n",
        "                total_drop_qty += qty\n",
        "\n",
        "        # [v29.0] Calculate backtest only after rookie phase\n",
        "        if not is_rookie_loc and last and last['type'] == 'REAL':\n",
        "             efficiency_checker.analyze_sku(last, mean, current_interval)\n",
        "\n",
        "    # --- Loop End, Summary ---\n",
        "    advisor_data_collector[location_name] = {'sku_stats': current_loc_stats, 'total_drop_qty': total_drop_qty}\n",
        "\n",
        "    # [v29.0] Call Advisor, passing is_rookie_loc\n",
        "    advisor = OperationsAdvisor()\n",
        "    advice_messages = advisor.analyze(current_loc_stats, current_interval, is_rookie=is_rookie_loc)\n",
        "\n",
        "    # [v29.0] Get Efficiency Score\n",
        "    efficiency_score = efficiency_checker.get_score() if not is_rookie_loc else None\n",
        "\n",
        "    sorted_periods_by_date = sorted(period_stats.values(), key=lambda x: x['date'], reverse=True)\n",
        "    sales_history_list = []\n",
        "    for p_data in sorted_periods_by_date:\n",
        "        d_sales = 0.0\n",
        "        if p_data['days'] > 0: d_sales = round(p_data['sales'] / p_data['days'], 2)\n",
        "        sales_history_list.append(d_sales)\n",
        "        if len(sales_history_list) >= 10: break\n",
        "\n",
        "    valid_data_count = len(valid_dates_found)\n",
        "\n",
        "    return location_name, target_date.strftime(\"%d%m%Y\"), results, is_rookie_loc, last_visit_str, sales_history_list, advice_messages, valid_data_count, full_scan_list, efficiency_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIfmEa2IAYSi",
        "outputId": "9d5c35dc-9a89-4531-8aa5-10e088c2e73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core_logic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import config\n",
        "importlib.reload(config)\n",
        "\n",
        "from data_utils import parse_strict_date, load_json, load_sku_map\n",
        "from core_logic import process_single_location_block\n",
        "from storage_manager import update_archives_and_save\n",
        "\n",
        "def generate_weekly_plan():\n",
        "    print(f\"ðŸ“… === Smart Weekly Scheduler ({config.VERSION}) ===\\n\")\n",
        "\n",
        "    # ... (File loading logic remains same)\n",
        "    today = datetime.datetime.now()\n",
        "    today = today.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    start_of_week = today - timedelta(days=today.weekday())\n",
        "    end_of_week = start_of_week + timedelta(days=6)\n",
        "\n",
        "    xlsx_files = [f for f in os.listdir('.') if f.endswith('.xlsx') and \"IMS\" in f and not f.startswith('~$')]\n",
        "    if not xlsx_files: return\n",
        "    xlsx_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "    target_file = xlsx_files[0]\n",
        "    abs_file_path = os.path.abspath(target_file)\n",
        "    sku_map = load_sku_map(abs_file_path)\n",
        "\n",
        "    intervals = {}\n",
        "    distributions = {}\n",
        "    all_locations = set()\n",
        "\n",
        "    xls = None\n",
        "    try: xls = pd.ExcelFile(abs_file_path)\n",
        "    except: return\n",
        "\n",
        "    try:\n",
        "        config_sheet = None\n",
        "        for s in xls.sheet_names:\n",
        "            if 'location' in s.lower(): config_sheet = s; break\n",
        "        if config_sheet:\n",
        "            ldf = pd.read_excel(xls, sheet_name=config_sheet, header=None)\n",
        "            n_idx = -1; i_idx = -1; d_idx = -1\n",
        "            for i, r in ldf.iterrows():\n",
        "                row_str = str(r.values).lower()\n",
        "                if 'location_id' in row_str: n_idx = i\n",
        "                if 'interval' in row_str: i_idx = i\n",
        "                if 'distribution' in row_str: d_idx = i\n",
        "            if n_idx != -1 and i_idx != -1:\n",
        "                names = ldf.iloc[n_idx]; vals = ldf.iloc[i_idx]\n",
        "                dists = ldf.iloc[d_idx] if d_idx != -1 else None\n",
        "                for c in range(len(ldf.columns)):\n",
        "                    l_raw = names[c]\n",
        "                    if pd.notna(l_raw) and str(l_raw).strip() != '' and str(l_raw).lower() != 'location_id':\n",
        "                        l = str(l_raw).strip(); v = str(vals[c]).strip().lower()\n",
        "                        all_locations.add(l)\n",
        "                        d = 7\n",
        "                        if v == '2': d = 14\n",
        "                        elif v == '4': d = 28\n",
        "                        elif '5x5' in v: d = 7\n",
        "                        intervals[l] = d\n",
        "                        dist_val = \"Other\"\n",
        "                        if dists is not None and c < len(dists):\n",
        "                            raw_dist = dists[c]\n",
        "                            if pd.notna(raw_dist): dist_val = str(raw_dist).strip()\n",
        "                        distributions[l] = dist_val\n",
        "    except: pass\n",
        "\n",
        "    print(\"ðŸ” Scanning Excel sheets and processing...\")\n",
        "    shadow = load_json(config.SHADOW_LEDGER_FILE)\n",
        "    advisor_data = {}\n",
        "    calculated_results = []\n",
        "    found_locations = set()\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        if \"location\" in sheet_name.lower() or \"sku\" in sheet_name.lower() or \"sheet\" in sheet_name.lower(): continue\n",
        "        df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
        "        loc_indices = []\n",
        "        for i, row in df.iterrows():\n",
        "            if 'Location_Id' in \" \".join([str(x) for x in row.values]): loc_indices.append(i)\n",
        "\n",
        "        for i in range(len(loc_indices)):\n",
        "            start_row = loc_indices[i]\n",
        "            end_row = loc_indices[i+1] if i < len(loc_indices)-1 else len(df)\n",
        "            blk = df.iloc[start_row:end_row].copy()\n",
        "            loc_row = df.iloc[start_row]\n",
        "            loc_name = None\n",
        "            for col_idx, cell_val in enumerate(loc_row):\n",
        "                if str(cell_val).strip() == 'Location_Id':\n",
        "                    if col_idx + 1 < len(loc_row): loc_name = str(loc_row[col_idx+1]).strip()\n",
        "                    break\n",
        "            if not loc_name: continue\n",
        "            found_locations.add(loc_name)\n",
        "\n",
        "            date_row = df.iloc[start_row - 1] if start_row > 0 else loc_row\n",
        "            last_d = None; valid_ds = []\n",
        "            for c in range(len(df.columns)):\n",
        "                d = parse_strict_date(date_row.iloc[c])\n",
        "                if d: valid_ds.append(d)\n",
        "            if valid_ds: last_d = max(valid_ds)\n",
        "            if not last_d: continue\n",
        "\n",
        "            inter = intervals.get(loc_name, config.DEFAULT_INTERVAL)\n",
        "            due = last_d + timedelta(days=inter)\n",
        "            status = \"\"; is_active = False\n",
        "\n",
        "            # [v29.1] Overdue Logic Update:\n",
        "            # If overdue, set repl date to \"today\", and pass as Override\n",
        "            target_date_override = None\n",
        "\n",
        "            if due < start_of_week:\n",
        "                status = \"ðŸ”´ Overdue\"\n",
        "                is_active = True\n",
        "                due = datetime.datetime.now() # Correct to today\n",
        "                target_date_override = due\n",
        "            elif start_of_week <= due <= end_of_week:\n",
        "                status = \"ðŸŸ¢ Planned\"\n",
        "                is_active = True\n",
        "                # Pass calculated date for consistency\n",
        "                target_date_override = due\n",
        "            else:\n",
        "                status = \"âšªï¸ Skipped\"\n",
        "                is_active = False\n",
        "\n",
        "            ln, ld, res, is_new, last_visit, sales_hist, advice, data_count, full_list, eff_score = process_single_location_block(\n",
        "                blk, loc_name, date_row, sku_map, {}, {}, shadow, advisor_data, current_interval=inter, dry_run=(not is_active),\n",
        "                target_date_override=target_date_override # Pass param\n",
        "            )\n",
        "\n",
        "            calculated_results.append({\n",
        "                'loc': ln, 'dist': distributions.get(ln, \"Other\"), 'status': status, 'date': due,\n",
        "                'res': res if is_active else [], 'is_new': is_new, 'last_visit': last_visit,\n",
        "                'sales_hist': sales_hist, 'advice': advice,\n",
        "                'data_count': data_count, 'full_list': full_list,\n",
        "                'eff_score': eff_score\n",
        "            })\n",
        "\n",
        "    dead_locations = all_locations - found_locations\n",
        "    for dead_loc in dead_locations:\n",
        "        calculated_results.append({'loc': dead_loc, 'dist': distributions.get(dead_loc, \"Other\"), 'status': \"âš«ï¸ Dead\", 'date': datetime.datetime.now(), 'res': [], 'is_new': False, 'last_visit': \"N/A\", 'sales_hist': [], 'advice': [], 'data_count': \"N/A\", 'full_list': [], 'eff_score': None})\n",
        "\n",
        "    if not calculated_results: return\n",
        "    fname = f\"Weekly_Plan_{start_of_week.strftime('%Y%m%d')}.xlsx\"\n",
        "    print(f\"ðŸ’¾ Generating Excel Report: {fname}\")\n",
        "\n",
        "    try:\n",
        "        with pd.ExcelWriter(fname, engine='xlsxwriter') as writer:\n",
        "            summary_rows = []\n",
        "            status_order = {\"ðŸ”´\": 0, \"ðŸŸ¢\": 1, \"âšªï¸\": 2, \"âš«ï¸\": 3}\n",
        "            calculated_results.sort(key=lambda x: status_order.get(x['status'][0], 99))\n",
        "\n",
        "            for item in calculated_results:\n",
        "                due_str = item['date'].strftime('%Y-%m-%d')\n",
        "                if \"Dead\" in item['status']: due_str = \"N/A\"\n",
        "\n",
        "                eff = item.get('eff_score')\n",
        "                eff_str = \"N/A (Rookie)\"\n",
        "                waste_str = \"-\"\n",
        "                missed_str = \"-\"\n",
        "                rating_str = \"-\"\n",
        "\n",
        "                if eff:\n",
        "                    eff_str = f\"{eff['score_pct']:.0%}\"\n",
        "                    waste_str = f\"{eff['waste']:.0f}\"\n",
        "                    missed_str = f\"{eff['missed']:.1f}\"\n",
        "                    rating_str = eff['rating']\n",
        "\n",
        "                row_data = {\n",
        "                    'Distribution': item['dist'], 'Status': item['status'], 'Location': item['loc'],\n",
        "                    'Last Visit': item['last_visit'], 'Due Date': due_str,\n",
        "                    'Rookie Mode': 'Yes' if item['is_new'] else 'No',\n",
        "                    'Efficiency': eff_str,\n",
        "                    'Over-Carry': waste_str,\n",
        "                    'Under-Carry': missed_str,\n",
        "                    'Rating': rating_str\n",
        "                }\n",
        "                for i in range(10):\n",
        "                    val = item['sales_hist'][i] if i < len(item['sales_hist']) else \"\"\n",
        "                    row_data[f'Daily Sales (T-{i+1})'] = val\n",
        "                summary_rows.append(row_data)\n",
        "            pd.DataFrame(summary_rows).to_excel(writer, sheet_name=\"Weekly Overview\", index=False)\n",
        "\n",
        "            # Format Overview Page\n",
        "            ws_sum = writer.sheets[\"Weekly Overview\"]\n",
        "            ws_sum.set_column('A:A', 15); ws_sum.set_column('B:B', 20); ws_sum.set_column('C:C', 25)\n",
        "            ws_sum.set_column('G:J', 12)\n",
        "\n",
        "            dist_groups = {}\n",
        "            for item in calculated_results:\n",
        "                if not item['res']: continue\n",
        "                raw_dist = str(item['dist']).strip(); d_name = raw_dist.title() if raw_dist else \"Other\"\n",
        "                if d_name not in dist_groups: dist_groups[d_name] = []\n",
        "                dist_groups[d_name].append(item)\n",
        "\n",
        "            count_created = 0\n",
        "            fmt_header = writer.book.add_format({'bold': True, 'font_color': 'blue', 'font_size': 14})\n",
        "            fmt_col_header = writer.book.add_format({'bold': True, 'bottom': 1})\n",
        "            fmt_warning = writer.book.add_format({'font_color': 'red', 'bold': True})\n",
        "            fmt_full_title = writer.book.add_format({'bold': True, 'bg_color': '#E0E0E0'})\n",
        "\n",
        "            for d_name, items in dist_groups.items():\n",
        "                sheet_title = str(d_name)[:30].replace('/','').replace('[','').replace(']','')\n",
        "                if sheet_title == \"\": sheet_title = \"Other\"\n",
        "                worksheet = writer.book.add_worksheet(sheet_title)\n",
        "                writer.sheets[sheet_title] = worksheet\n",
        "                current_col = 0\n",
        "\n",
        "                for item in items:\n",
        "                    header_text = f\"{item['loc']}\\nDue: {item['date'].strftime('%Y-%m-%d')}\"\n",
        "                    worksheet.merge_range(0, current_col, 0, current_col+2, header_text, fmt_header)\n",
        "\n",
        "                    df_res = pd.DataFrame(item['res'], columns=['Item', 'Qty', 'Cap'])\n",
        "                    worksheet.write(1, current_col, \"Item\", fmt_col_header)\n",
        "                    worksheet.write(1, current_col+1, \"Qty\", fmt_col_header)\n",
        "                    worksheet.write(1, current_col+2, \"Cap\", fmt_col_header)\n",
        "\n",
        "                    last_data_row = 2\n",
        "                    for row_num, row_data in enumerate(df_res.values):\n",
        "                        worksheet.write(2 + row_num, current_col, row_data[0])\n",
        "                        worksheet.write(2 + row_num, current_col+1, row_data[1])\n",
        "                        worksheet.write(2 + row_num, current_col+2, row_data[2])\n",
        "                        last_data_row = 2 + row_num\n",
        "\n",
        "                    if item['advice']:\n",
        "                        worksheet.write(last_data_row + 2, current_col, \"ðŸ’¡ Advice:\", fmt_warning)\n",
        "                        for adv_idx, advice_text in enumerate(item['advice']):\n",
        "                            worksheet.write(last_data_row + 3 + adv_idx, current_col, advice_text)\n",
        "                        last_data_row += len(item['advice']) + 3\n",
        "                    else: last_data_row += 3\n",
        "\n",
        "                    full_start_row = last_data_row + 2\n",
        "                    worksheet.write(full_start_row, current_col, \"ðŸ“‹ Full Check\", fmt_full_title)\n",
        "                    worksheet.write(full_start_row + 1, current_col, \"Item\", fmt_col_header)\n",
        "                    worksheet.write(full_start_row + 1, current_col+1, \"Qty\", fmt_col_header)\n",
        "                    worksheet.write(full_start_row + 1, current_col+2, \"Cap\", fmt_col_header)\n",
        "\n",
        "                    df_full = pd.DataFrame(item['full_list'], columns=['Item', 'Qty', 'Cap'])\n",
        "                    for row_num, row_data in enumerate(df_full.values):\n",
        "                        worksheet.write(full_start_row + 2 + row_num, current_col, row_data[0])\n",
        "                        worksheet.write(full_start_row + 2 + row_num, current_col+1, row_data[1])\n",
        "                        worksheet.write(full_start_row + 2 + row_num, current_col+2, row_data[2])\n",
        "\n",
        "                    worksheet.set_column(current_col, current_col, 30)\n",
        "                    worksheet.set_column(current_col+1, current_col+1, 6)\n",
        "                    worksheet.set_column(current_col+2, current_col+2, 6)\n",
        "                    worksheet.set_column(current_col+3, current_col+3, 2)\n",
        "                    current_col += 4\n",
        "                    count_created += 1\n",
        "\n",
        "        update_archives_and_save(shadow, advisor_data)\n",
        "        print(f\"âœ… Success! Report generated: {fname}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error generating Excel: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    generate_weekly_plan()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlAxaFCrAaua",
        "outputId": "9126fec6-35a9-40fe-e348-3823db9c18cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsyVc74ABLtd",
        "outputId": "38ba2889-7f1d-4359-bc42-1e1b7f8f6366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“… === Smart Weekly Scheduler (v29.1 (Feat: Overdue Auto-Correction)) ===\n",
            "\n",
            "âœ… SKU Map loaded successfully: Contains 120 products\n",
            "ðŸ” Scanning Excel sheets and processing...\n",
            "ðŸ’¾ Generating Excel Report: Weekly_Plan_20260105.xlsx\n",
            "âœ… All background archives safely updated (Shadow, History, Graveyard, Visit).\n",
            "âœ… Success! Report generated: Weekly_Plan_20260105.xlsx\n"
          ]
        }
      ]
    }
  ]
}